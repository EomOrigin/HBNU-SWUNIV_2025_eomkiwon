<div align="center">

# Hi I'm Kiwon Eom, I'm a dreamer of studying ML/DL. <br>Also, I'm a light fan of team Barcelona.👋

![header](https://capsule-render.vercel.app/api?type=wave&color=00ff00&height=300&section=header&text=kiwon_official&fontSize=80)


</div>


## 📫Contact
[![Gmail Badge](https://img.shields.io/badge/-Gmail-c14438?style=flat-square&logo=Gmail&logoColor=white)](mailto:eomkiwon1528@gmail.com) 
[![Tech Blog Badge](https://img.shields.io/badge/-Tech%20blog-black?style=flat-square&logo=blogger&logoColor=white)](https://blog.naver.com/vigor1528)
[![Instagram Badge](https://img.shields.io/badge/-Instagram-5851DB?style=flat-square&logo=instagram&logoColor=white)](https://www.instagram.com/eomki1_1?igsh=c216MzFpMnpkbnQ5&utm_source=qr)

## 🔭About me

### 😄 Interests
- Computer Vision
- SAR Object Detection / Segmentation
- Digital Forensics
- Machine Learning / Deep Learning

### :mortar_board: Education
- B.S. in School of Computer Science, Hanbat National University `2023.03 ~ `

### 🔭 Internships
- Undergraduate Researcher — [AIM Lab](https://sites.google.com/view/aim-lab-hbnu/home), Hanbat National University `2024.02 ~`

### 📚 Publications

- **ViT-based Deepfake Detection via Regional Emphasis Weights on Eyes, Nose, and Mouth.** _Digital Forensics Research, 2025, vol.19, no.4, pp.116–131. [[KCI Link](https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART003243484)]





### ⚡ ExtraCurricular Activity

- Team Member of Volunteer Club 'ssnb_hbnu' ```2025.03 ~ 2025.08```

### Challenge
- How can we design E.O. (Electro-Optical)→SAR image-transfer and domain-adaptation methods that preserve SAR-specific physical and statistical properties (e.g., speckle, polarization, radiometric and phase characteristics) using physics-aware generative models and domain-aware learning to improve SAR object detection and segmentation?
  
### Additional Information
If you want to see more information about me, here is my [**CV 📥**]()

<div>

## ⚡ Skills ⚡

  ### 💻 Programming Language
  <img src="https://img.shields.io/badge/python-3776AB?style=flat-square&logo=python&logoColor=white">
  <img src="https://img.shields.io/badge/Java-276DC3?style=flat-square&logo=OpenJDK&logoColor=white">
  <img src="https://img.shields.io/badge/C-276DC3?style=flat-square&logo=C&logoColor=white">

  ### 📚 Stacks
  <img src="https://img.shields.io/badge/Pandas-150458?style=flat-square&logo=Pandas&logoColor=white">
  <img src="https://img.shields.io/badge/Numpy-150458?style=flat-square&logo=Numpy&logoColor=white">
  <img src="https://img.shields.io/badge/scikit-learn-F7931E?style=flat&logo=scikit-learn&logoColor=white"/>
  <img src="https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=TensorFlow&logoColor=white"/> 
  <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=PyTorch&logoColor=white">
  <img src="https://img.shields.io/badge/Matplotlib-00ffff?style=flat-square&logo=Matplotlib&logoColor=black">
  <img src="https://img.shields.io/badge/Keras-D00000?style=flat-square&logo=Keras&logoColor=white"> <br/> 
  <img src="https://img.shields.io/badge/OpenCV-005C3C?style=flat-square&logo=opencv&logoColor=white" alt="opencv"/>
  <img src="https://img.shields.io/badge/MMDetection-151515?style=flat-square&logo=python&logoColor=white" alt="mmdetection"/>
  
  ### 🛠 Tools
  <img src="https://img.shields.io/badge/VS%20Code-007ACC?style=flat-square&logo=visual-studio-code&logoColor=white" alt="vscode"/>
  <img src="https://img.shields.io/badge/Cursor-111827?style=flat-square&logo=cursor&logoColor=white" alt="cursor"/>
  <img src="https://img.shields.io/badge/Jupyter-F37626?style=flat-square&logo=Jupyter&logoColor=white"/>  
  <img src="https://img.shields.io/badge/Anaconda-44A833?style=flat-square&logo=Anaconda&logoColor=white"/> 
  <img src="https://img.shields.io/badge/Docker-2496ED?style=flat-square&logo=docker&logoColor=white" alt="docker"/>
  <img src="https://img.shields.io/badge/W%26B-FF7B00?style=flat-square&logo=wandb&logoColor=white" alt="wandb"/>


 </div>
 
## 🏆 Dacon competitions
profile : [my dacon profile page](https://dacon.io/myprofile/497780/home), [my kaggle profile page](https://www.kaggle.com/kiwoneom) I'm still have so much to develop.
- 🥉 2024년 SW중심대학 공동 AI 경진대회 예선 (팀명 : DSDK) - **Finished 2nd (2/13)** [[overview](https://www.kaggle.com/competitions/hbnu-fake-audio-detection-competition/overview)]
- 🥉 2025년 SW중심대학 공동 AI 경진대회 예선 (팀명 : K2E) - **Finished 5th (5/17)** [[overview](https://www.kaggle.com/competitions/fake-text-detection-competition/overview)]
- 🥉 2025년 SW중심대학 공동 AI 경진대회 본선 (팀명 : K2E) - **Top 6% (16/271)** [[overview](https://dacon.io/competitions/official/236473/overview/description)]


## 🏆 Other Competitions
- 제6회 2024 연구개발특구 AI SPARK 챌린지(위성 다분광 영상 기반 산불 탐지) **대상, Top 1 in Final** [[overview](https://aifactory.space/task/2723/overview)]

 </div>

 ## 🚀 Projects
 
   ### Smart Face-Recognition Door Lock
   - Period: 2024.09 - 2024.12
   - Role: Team Lead
   - Tech stack: Python, Raspberry Pi 4, OpenCV, SQLite, MQTT 
   - Description: Developed a face-recognition retrofit for an existing door lock to enhance security: the system captures images at the door, performs on-device real-time face recognition, logs access events, and controls the lock mechanism via GPIO. Implemented photo capture for audit trails and optional remote alerts for unrecognized access attempts.

  ### Kaggle Challenge(Image Classification, AI, 2025-Spring)
   - Period: 2025.03 - 2025.06
   - Role: Team Lead
   - Tech stack: Python, Pytorch
   - Description: DTrained an image-classification model to determine whether a given image is a live (genuine) human face or a spoof (attack) face, achieving 98.33% accuracy
   - [[overview](https://www.kaggle.com/competitions/image-classification-hbnu-ai-2025-spring/overview)]


  # 한밭대학교 컴퓨터공학과 2025 포트폴리오경진대회
## 국립한밭대학교 컴퓨터공학과 20231203 엄기원

## 주제 
- 눈·코·입 영역 강조 가중치를 활용한 ViT 기반 딥페이크 탐지 기법 연구
  
## Project Background
  - ### 개요
   본 프로젝트는 Vision Transformer(ViT) 기반의 딥페이크 탐지 성능을 향상시키기 위해 눈·코·입(eyes, nose, mouth) 영역에 학습 가능한 강조 가중치(Region Weight Module, RWM)를 부여하는 기법을 제안하고 구현한 연구입니다. 얼굴 랜드마크는 MediaPipe를 통해 추출하고, 입력 이미지를 16×16 패치(총 196개)로 분할한 뒤 해당 패치들에 부위별 가중치를 적용하여 ViT의 패치 임베딩 단계에서 국소적 합성 흔적(artifact)을 더 민감하게 학습하도록 설계했습니다. 이 방법은 ViT가 갖는 전역 정보에는 강하지만 국소적 왜곡을 놓치기 쉬운 한계를 보완합니다.


<img width="519" height="217" alt="image" src="https://github.com/user-attachments/assets/99427343-8805-489f-87a7-087aa8ca904f" />

    
  - ### 필요성
   최근 딥페이크 기술의 고도화와 함께 피해를 목적으로 한 딥페이크 범죄(사칭, 금전적 사기, 명예훼손 등) 가 증가하고 있어, 단순 감시·탐지 수준을 넘어 실사용자 보호를 목표로 한 예방적·실시간 대응 기술의 도입이 시급합니다. 본 연구의 RWM 기반 딥페이크 탐지 기법은 이러한 위협에 대해 현장 적용 가능한 실시간 검증 도구로 활용될 잠재력이 높습니다.
    
## 프로젝트 내용

<img width="504" height="250" alt="image" src="https://github.com/user-attachments/assets/f9043ce0-699a-436d-aff5-6b0bc7a6c7a5" />

 - ### 구현 내용
  1.  **데이터 파이프라인 & 전처리**
      -   FaceForensics++ 등 공개 딥페이크 데이터셋을 수집하고, 학습/검증/테스트 split을 구성
      -   프레임 추출, 얼굴 검출/정렬, 패치 분할(16×16) 등 전처리 스크립트를 작성하여 일관된 입력 형식을 작성

  2.  **RWM 모듈**
      -   RegionAttention 은 입력 이미지(224×224 등)를 ViT 패치 단위(예: 16×16 → 14×14 = 196 patch)로 분할한 뒤, 눈/코/입 영역에 대한 가중치(스칼라 또는 패치별 벡터) 를 만들어 각 패치 토큰에 곱해주는 모듈
      -   image_size와 patch_size로 grid_rows/grid_cols(R, C)와 patch_h/patch_w(실패치 크기)를 계산하고, 얼굴 랜드마크 좌표를 받아, 각 landmark를 패치 인덱스 (r,c)로 변환해 region별 mask(0/1)를 생성

  3.  **최종 모델 구조**
      -   ViTWithRegionBias 클래스가 timm의 vit_small_patch16_224를 불러와 RegionAttention을 생성하고 패치 임베딩 단계 직후에 weight_map을 적용 

- ### 실험 결과 
  1.  **Baseline과 RWM 적용 후 및 가중치 모드 별 Test 성능**
     - 본 논문의 제안 기법인 Multiple Learnable 가중치는 Baseline 대비 최대 7.94%의 ACER 개선을 이끌어냈으며, AUC 역시 87.91%에서 90.90%로 증가하는 성능 향상이 나타났습니다. 
     
     <img width="661" height="193" alt="image" src="https://github.com/user-attachments/assets/d756ae27-f75d-4e4a-b0d1-3bb8ff5a5e17" />

  2.  **연산 효율성 및 실시간 추론 적용**: 랜드마크 조회와 weight map 생성 연산을 포함하더라도 RWM의 평균 추론시간은 약 12.01 ms로, 실시간·저지연 환경에서도 적용 가능함을 보였습니다.
  3.  **설명 가능성(Explainability) 제고**: RWM이 학습하는 패치별 가중치(heatmap)를 통해 모델이 어느 얼굴 부위를 근거로 판별했는지 시각적으로 확인할 수 있습니다. 이는 딥페이크 판정의 근거를 제공해 포렌식 분석·디버깅·사용자 신뢰 확보에 도움을 주며, 규제·컴플라이언스 대응이나 결과 해석이 필요한 실제 서비스 환경에서 중요한 장점이 됩니다
     
- ### 기대 효과
  1.  **딥페이크 탐지 성능 향상**: 딥페이크는 전체 얼굴이 아닌 눈·코·입 주변에서 미세한 합성 왜곡이 주로 발생하므로, 전역적 패치 처리만 하는 기존 ViT 구조는 이러한 미세 징후를 약하게 학습합니다. 해당 기법을 통해 이러한 단점을 보완하여 더욱 강건한 딥페이크 탐지가 가능합니다. 
  2.  **연산 효율성 및 실시간 추론 적용**: 랜드마크 조회와 weight map 생성 연산을 포함하더라도 RWM의 평균 추론시간은 약 12.01 ms로, 실시간·저지연 환경에서도 적용 가능함을 보였습니다.
  3.  **설명 가능성(Explainability) 제고**: RWM이 학습하는 패치별 가중치(heatmap)를 통해 모델이 어느 얼굴 부위를 근거로 판별했는지 시각적으로 확인할 수 있습니다. 이는 딥페이크 판정의 근거를 제공해 포렌식 분석·디버깅·사용자 신뢰 확보에 도움을 주며, 규제·컴플라이언스 대응이나 결과 해석이 필요한 실제 서비스 환경에서 중요한 장점이 됩니다.

## 개발환경
- ### 개발 언어 :
  - Python
- ### 프레임워크 및 라이브러리 :
  - PyTorch, torchvision, timm, MediaPipe, OpenCV, einops, numpy, pandas, albumentations, matplotlib, wandb
- ### API 및 협업 도구 :
  - Git, GitHub, Docker, nvidia-docker (nvidia-container-toolkit), WSL (Ubuntu), CUDA
 

[![KiwonEom's github stats](https://github-readme-stats.vercel.app/api?username=EomOrigin&show_icons=true&theme=tokyonight)](https://github.com/EomOrigin)
![Top Langs](https://github-readme-stats.vercel.app/api/top-langs/?username=EomOrigin&layout=compact&theme=tokyonight)


  <!--



- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...
-->
